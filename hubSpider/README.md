### xvideos视频爬虫

​	曾经有一份爬取pornhub的爬虫摆在我面前，但我没有珍惜，等到pornhub大清洗过后我才追悔莫及，尘世间最痛苦的事莫过于此。如果上天可以给我再来一次的机会，我一定全爬下来放到数据库中。

​	本来想爬pornhub的，但是pornhub这不经历了一次灾难后，把好多视频都删了而且现存的视频好多都需要付费下载。所以这次将矛头对准了另一家free的[网站](https://www.xvideos.com)。

​	**功能**

​	通过用户给出的关键词，爬取该关键词搜索到的所有的视频，并将这些视频以.mp4的的形式保存在本地电脑上（未来打算接入mongodb数据库，存在数据库中，以免未来再发生pornhub事件）

​	**环境**

1. windows10
2. python3.7

​	**使用方法**

1. 首先需要一个可以带动整个电脑科学上网的vpn或机场**这个大家自己找一下哈**
2. 需要使用到的库已经放在requirements.txt，使用pip安装的可以使用指令`pip install -r requirements.txt`。如果国内安装第三方库比较慢，可以使用以下指令进行清华源加速`pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/`
3. 具备了以上两个条件之后，我们就可以进入`search_parser.py`文件中将`key`修改成你要搜索的内容即可，然后运行`search_parser.py`即可，在运行之后应该会在video目录下得到一个`video_url.txt`文件，里面就是搜索到的所有的与关键词相关的页面了。
4. 之后，我们只需要直接进入`downloader.py`文件，运行即可。此时会在控制台让你分别输入，邮箱和密码（没有xvideos网站账号的去网站注册一个就OK，免费的）。输入邮箱和密码后，首先会解析刚刚获取到的所有的页面，而后会逐个下载，下载的mp4文件会存在video目录下。

> **注意** 在spiders目录下会有一个`cookies.txt`文件，其中保存着你登录后留下的cookies，是下载视频的必须品，所以不要轻易删掉该文件。

​	**技术栈**

1. requests

2. json

3. parsel（xpath筛选器）

4. os

5. cookiejar（模拟登陆保存cookies）

6. scrapy（自动迭代）~~上次自己写了一个递归迭代感觉太麻烦了，还是直接用现成的香~~

​	**未完成功能**

1. 接入mongodb数据库
2. 多线程下载（现在的下载速度着实慢）

​	**进步（相较于之前）**

1.  实现了模拟登陆
2. 完成了scrapy框架和别的库的交叉使用
3. 实现了视频的保存（之前B站视频时通过第三方库下载的）

